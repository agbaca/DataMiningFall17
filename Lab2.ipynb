{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Task 1 Vehicles with over 100K Kilometers/Task 2 Body Type.\n",
    "IN this lab we will be using a dataset found on Kaggle.com about vehcile advertisements in the Czech republic and Germany.  For task 1 we will be focusing on predicting if a vehicle has over 100K kilometers. For task 2 we will be focusing on predicting the different body types of the vehicles.\n",
    "\n",
    "    Scott Gozdzialski\n",
    "    Adam Baca\n",
    "    Zoheb Allam\n",
    "    Ethan Graham\n",
    "    \n",
    "    The data can be found https://www.kaggle.com/mirosval/personal-cars-classifieds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roughly 3.5 Million rows and the following columns:\n",
    "\n",
    "- maker - The manufacturer of the vehicle\n",
    "- model - The distinct model of the vehicle\n",
    "- mileage - in KM (our Response variable)\n",
    "- manufacture_year\n",
    "- engine_displacement - in cc\n",
    "- engine_power - in kW\n",
    "- body_type - Coupe, van, sedan, etc.\n",
    "- color_slug - main color of the vehicle\n",
    "- stk_year - year of the last emission control\n",
    "- transmission - automatic or manual\n",
    "- door_count\n",
    "- seat_count\n",
    "- fuel_type - gasoline, diesel, cng, lpg, electric\n",
    "- date_created - when the ad was scraped\n",
    "- date_last_seen - when the ad was last seen. Our policy was to remove all ads older than 60 days\n",
    "- price_eur - list price converted to EUR\n",
    "\n",
    "The first step is to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the file of 3.5 Million records we will parse it down to 81000 usable records\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "path = \"~\\\\Desktop\\\\Cars.csv\"\n",
    "\n",
    "df = pd.read_csv(path,sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to have to clean the data.  As can be seen below most of the data is object type wich will not work for our classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3552912 entries, 0 to 3552911\n",
      "Data columns (total 16 columns):\n",
      "maker                  object\n",
      "model                  object\n",
      "mileage                float64\n",
      "manufacture_year       float64\n",
      "engine_displacement    float64\n",
      "engine_power           float64\n",
      "body_type              object\n",
      "color_slug             object\n",
      "stk_year               object\n",
      "transmission           object\n",
      "door_count             object\n",
      "seat_count             object\n",
      "fuel_type              object\n",
      "date_created           object\n",
      "date_last_seen         object\n",
      "price_eur              float64\n",
      "dtypes: float64(5), object(11)\n",
      "memory usage: 433.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we change the date the ad was created and the date it was removed to a interger of the number of days the ad ran. then we drop the columns we will not be using. Stk-year is very close to model year, model takes up to much memory seperate and is unworkable.  Finally we will drop all the rows with NAs.  With 3.5 rows we have plenty to use after removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the date varibles into a delta between and type int\n",
    "df.date_created = pd.to_datetime(df['date_created'])\n",
    "df.date_last_seen = pd.to_datetime(df['date_last_seen'])\n",
    "df['total_days'] = df['date_last_seen'] - df['date_created']\n",
    "df.total_days = df['total_days'].dt.days.astype(int)\n",
    "\n",
    "df.drop(['stk_year','model','date_created','date_last_seen'], axis=1, inplace=True)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert door count and seat count to ints, and remove eronious information.  There are no vehicles with a 10cc engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.door_count = df.door_count.replace('None','0')\n",
    "df.door_count = df.door_count.astype(int)\n",
    "df.seat_count = df.door_count.replace('None','0')\n",
    "df.seat_count = df.door_count.astype(int)\n",
    "\n",
    "df = df.sort_values('engine_displacement', ascending=False)\n",
    "df = df[:82088]\n",
    "\n",
    "df = df.sort_values('engine_power', ascending=False)\n",
    "df = df[:81500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "This is where the dataframe for task one and task two deviate from each other.  Task one will one hot encode body type to predict milage.\n",
    "\n",
    "Then, we OneHotEncode maker, body type,color slug, and fuel type. We turn Transmision to binary (1,0) variable. Finally we remove the columns that we OneHotEncoded.  We also make a back up dataframe incase we make a mistake we can go backto this point without rerunning everything above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df\n",
    "\n",
    "tmp_df = pd.get_dummies(df.maker,prefix='Maker')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.body_type,prefix='Body type')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.color_slug,prefix='Color')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.fuel_type,prefix='Fuel')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "df['manual'] = df.transmission=='man' \n",
    "df.manual = df.manual.astype(np.int)\n",
    "\n",
    "df.drop(['body_type','color_slug','fuel_type','maker','transmission'], axis= 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 81500 entries, 3215366 to 3153910\n",
      "Data columns (total 83 columns):\n",
      "mileage                   81500 non-null float64\n",
      "manufacture_year          81500 non-null float64\n",
      "engine_displacement       81500 non-null float64\n",
      "engine_power              81500 non-null float64\n",
      "door_count                81500 non-null int32\n",
      "seat_count                81500 non-null int32\n",
      "price_eur                 81500 non-null float64\n",
      "total_days                81500 non-null int32\n",
      "Maker_alfa-romeo          81500 non-null uint8\n",
      "Maker_aston-martin        81500 non-null uint8\n",
      "Maker_audi                81500 non-null uint8\n",
      "Maker_bentley             81500 non-null uint8\n",
      "Maker_bmw                 81500 non-null uint8\n",
      "Maker_chevrolet           81500 non-null uint8\n",
      "Maker_chrysler            81500 non-null uint8\n",
      "Maker_citroen             81500 non-null uint8\n",
      "Maker_dacia               81500 non-null uint8\n",
      "Maker_dodge               81500 non-null uint8\n",
      "Maker_fiat                81500 non-null uint8\n",
      "Maker_ford                81500 non-null uint8\n",
      "Maker_honda               81500 non-null uint8\n",
      "Maker_hummer              81500 non-null uint8\n",
      "Maker_hyundai             81500 non-null uint8\n",
      "Maker_infinity            81500 non-null uint8\n",
      "Maker_isuzu               81500 non-null uint8\n",
      "Maker_jaguar              81500 non-null uint8\n",
      "Maker_jeep                81500 non-null uint8\n",
      "Maker_kia                 81500 non-null uint8\n",
      "Maker_lamborghini         81500 non-null uint8\n",
      "Maker_lancia              81500 non-null uint8\n",
      "Maker_land-rover          81500 non-null uint8\n",
      "Maker_lexus               81500 non-null uint8\n",
      "Maker_lotus               81500 non-null uint8\n",
      "Maker_maserati            81500 non-null uint8\n",
      "Maker_mazda               81500 non-null uint8\n",
      "Maker_mercedes-benz       81500 non-null uint8\n",
      "Maker_mini                81500 non-null uint8\n",
      "Maker_mitsubishi          81500 non-null uint8\n",
      "Maker_nissan              81500 non-null uint8\n",
      "Maker_opel                81500 non-null uint8\n",
      "Maker_peugeot             81500 non-null uint8\n",
      "Maker_porsche             81500 non-null uint8\n",
      "Maker_renault             81500 non-null uint8\n",
      "Maker_rolls-royce         81500 non-null uint8\n",
      "Maker_rover               81500 non-null uint8\n",
      "Maker_seat                81500 non-null uint8\n",
      "Maker_skoda               81500 non-null uint8\n",
      "Maker_smart               81500 non-null uint8\n",
      "Maker_subaru              81500 non-null uint8\n",
      "Maker_suzuki              81500 non-null uint8\n",
      "Maker_tesla               81500 non-null uint8\n",
      "Maker_toyota              81500 non-null uint8\n",
      "Maker_volkswagen          81500 non-null uint8\n",
      "Maker_volvo               81500 non-null uint8\n",
      "Body type_compact         81500 non-null uint8\n",
      "Body type_convertible     81500 non-null uint8\n",
      "Body type_coupe           81500 non-null uint8\n",
      "Body type_offroad         81500 non-null uint8\n",
      "Body type_other           81500 non-null uint8\n",
      "Body type_sedan           81500 non-null uint8\n",
      "Body type_stationwagon    81500 non-null uint8\n",
      "Body type_transporter     81500 non-null uint8\n",
      "Body type_van             81500 non-null uint8\n",
      "Color_beige               81500 non-null uint8\n",
      "Color_black               81500 non-null uint8\n",
      "Color_blue                81500 non-null uint8\n",
      "Color_bronze              81500 non-null uint8\n",
      "Color_brown               81500 non-null uint8\n",
      "Color_gold                81500 non-null uint8\n",
      "Color_green               81500 non-null uint8\n",
      "Color_grey                81500 non-null uint8\n",
      "Color_orange              81500 non-null uint8\n",
      "Color_red                 81500 non-null uint8\n",
      "Color_silver              81500 non-null uint8\n",
      "Color_violet              81500 non-null uint8\n",
      "Color_white               81500 non-null uint8\n",
      "Color_yellow              81500 non-null uint8\n",
      "Fuel_cng                  81500 non-null uint8\n",
      "Fuel_diesel               81500 non-null uint8\n",
      "Fuel_electric             81500 non-null uint8\n",
      "Fuel_gasoline             81500 non-null uint8\n",
      "Fuel_lpg                  81500 non-null uint8\n",
      "manual                    81500 non-null int32\n",
      "dtypes: float64(5), int32(4), uint8(74)\n",
      "memory usage: 10.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing mileage to a binary of milage over 100K Kilometers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['mileage_100K'] = df['mileage'] > 100000\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "\n",
    "y = df['mileage_100K'].values # get the labels we want\n",
    "del df['mileage_100K'] \n",
    "del df['mileage']# get rid of the class label\n",
    "X = df.values # use everything else to predict!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Task 2\n",
    "For task 2 we will be predicting the body type and leaving mileage as a consistant number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_df = pd.get_dummies(df2.maker,prefix='Maker')\n",
    "df2 = pd.concat((df2,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df2.color_slug,prefix='Color')\n",
    "df = pd.concat((df2,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df2.fuel_type,prefix='Fuel')\n",
    "df = pd.concat((df2,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "df2['manual'] = df2.transmission=='man' \n",
    "df2.manual = df2.manual.astype(np.int)\n",
    "\n",
    "df2.drop(['color_slug','fuel_type','maker','transmission'], axis= 1, inplace = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2 = df2['body_type'].values # get the labels we want\n",
    "del df2['body_type'] \n",
    "X2 = df2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "#dftest = df\n",
    "\n",
    "#selector = VarianceThreshold(.05)\n",
    "#selector.fit_transform(dftest)\n",
    "#idx = selector.get_support()\n",
    "#idx2 = idx.index()\n",
    "#idx2\n",
    "#dftest2 = dftest[:,idx2.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation part 2 - Final dataset.\n",
    "The final dataset constists of 81500 records with 82 columns.\n",
    "\n",
    "First we downloaded our dataset of car sales in the Czech Republic and Germany.  Most of it downloads as obect type so we changed door count and seat count to intergers.  \n",
    "\n",
    "We calculated the difference between when the advertisment started and was dropped and created a new variable of total days the ad ran.    \n",
    "\n",
    "We dropped the stk-year and model, stk-year is not needed since it is very similar to model year.  Model has to many classifications in the rows for us to be able to seperate it out.  When we tried we ran out of memory.\n",
    "\n",
    "#### Task 1\n",
    "\n",
    "Speaking of seperation the classifactions out, we spereated out Maker, Body type, Color, and Fuel type with one hot endoing. \n",
    "\n",
    "#### Task 2\n",
    "\n",
    "For the seperations in task 2, we seperated out Maker, Color, and Fuel Type with one hot encoding.  We left body type in the dataframe because this will now be our response variable.  Body type has 9 different classifaction we will be focusing on predicting.\n",
    "\n",
    "#### Both tasks\n",
    "We also, dropped any row with a NA value.  This was done for two reasons, first it removed useless rows that will mess with our classifiaction models, second since we started with 3.5M records dropping the rows with NAs left us with 81500 rows of usable data.  The entire 3.5M records would eat up the resources of our machines and 81500 records should be a large enough sampleset to properly capture the nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model and Evaluation 1 - Evaluation Metric.\n",
    "\n",
    "To evaluate our different classification methods, we will be examining the accuracy with which they can predict vehicles with over 100k miles (task 1) and vehicle body type (task 2). The accuracy of each method will tell us the percentage of our sample correctly classified (PCC), in otherwords the percent of true positives and true negatives. PCC is the most commonly used metric to assess overall model accuracy and is calculated without taking into account what kind of errors are made, meaning each error has the same weight. Since our models aim to predict features in cars and not something related to health care like cancer, we are not concerned about the different impacts the false positives and false negatives may have on our sample and therefore can be treated equally.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Evaluation 2 - dividing data\n",
    "\n",
    "We will be using 10 fold cross validation in order to divide our data into training and testing splits. Cross validation is when you divide a sample of data into subsets and then perform the analysis on a training subset and validate those results with the testing subset. Cross validation allows you to determine if the results of the model will generalize to an independent dataset and also can limit issues like overfitting. \n",
    "\n",
    "With 10 fold cross validation, the cross validation process is repeated 10 times with each of the subsamples being used only once for validation. The main advantages to repeating this process 10 times is that all observations are used for both training and validation and therefore we do not lose sample size which can affect modeling capabilities. \n",
    "\n",
    "In order to aggregate the results from the 10 fold cross validation we will take the PCC for each model and average it together to form an overall accuracy measure. This will enable us to get a more accurate estimate of each models performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Evaluation 3 - Model selection\n",
    "\n",
    "** Needs write up\n",
    "I say we do K nearest neighbors, random forest, and Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - task 1 (mileage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.428045638572\n",
      "Random Forest  1  accuracy 0.855109802478\n",
      "Random Forest  2  accuracy 0.87314439946\n",
      "Random Forest  3  accuracy 0.84713532082\n",
      "Random Forest  4  accuracy 0.847116564417\n",
      "Random Forest  5  accuracy 0.794601226994\n",
      "Random Forest  6  accuracy 0.772610136213\n",
      "Random Forest  7  accuracy 0.744263099767\n",
      "Random Forest  8  accuracy 0.801816173764\n",
      "Random Forest  9  accuracy 0.746717388637\n",
      "Average accuracy =  77.1055975112 +- 12.2282325962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50,random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    rf_y_hat = clf.predict(X_test)\n",
    "    rf_acc[iter_num] = mt.accuracy_score(y_test,rf_y_hat)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc.mean()*100, \"+-\", rf_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - task 2 (Body type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.587289903079\n",
      "Random Forest  1  accuracy 0.600785179733\n",
      "Random Forest  2  accuracy 0.596736596737\n",
      "Random Forest  3  accuracy 0.595632437738\n",
      "Random Forest  4  accuracy 0.56282208589\n",
      "Random Forest  5  accuracy 0.576196319018\n",
      "Random Forest  6  accuracy 0.571481163333\n",
      "Random Forest  7  accuracy 0.581298318812\n",
      "Random Forest  8  accuracy 0.656644987115\n",
      "Random Forest  9  accuracy 0.692232175727\n",
      "Average accuracy =  60.2111916718 +- 3.86970345172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50,random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc2 = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    rf_y_hat2 = clf.predict(X2_test)\n",
    "    rf_acc2[iter_num] = mt.accuracy_score(y2_test,rf_y_hat2)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc2[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc2.mean()*100, \"+-\", rf_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors - Task 1 (Mileage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.521899153478\n",
      "K Nearest Neighbors  1  accuracy 0.714513556619\n",
      "K Nearest Neighbors  2  accuracy 0.770580296896\n",
      "K Nearest Neighbors  3  accuracy 0.719175561281\n",
      "K Nearest Neighbors  4  accuracy 0.694969325153\n",
      "K Nearest Neighbors  5  accuracy 0.699386503067\n",
      "K Nearest Neighbors  6  accuracy 0.688428027979\n",
      "K Nearest Neighbors  7  accuracy 0.684501165787\n",
      "K Nearest Neighbors  8  accuracy 0.7229107866\n",
      "K Nearest Neighbors  9  accuracy 0.686219167996\n",
      "Average accuracy =  69.0258354486 +- 6.11926978358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "iter_num = 0\n",
    "knn_acc= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    knn_y_hat = clf.predict(X_test)\n",
    "    knn_acc[iter_num] = mt.accuracy_score(y_test,knn_y_hat)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc.mean()*100, \"+-\", knn_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neightbors Task 2 ( Body Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test 1,3,5, and 7 neighbors\n",
    "\n",
    "Staring with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.267574530732\n",
      "K Nearest Neighbors  1  accuracy 0.302416881364\n",
      "K Nearest Neighbors  2  accuracy 0.324745430009\n",
      "K Nearest Neighbors  3  accuracy 0.312722365354\n",
      "K Nearest Neighbors  4  accuracy 0.306871165644\n",
      "K Nearest Neighbors  5  accuracy 0.350552147239\n",
      "K Nearest Neighbors  6  accuracy 0.351454166155\n",
      "K Nearest Neighbors  7  accuracy 0.395385936925\n",
      "K Nearest Neighbors  8  accuracy 0.430114124432\n",
      "K Nearest Neighbors  9  accuracy 0.45465701313\n",
      "Average accuracy =  34.9649376099 +- 5.69017219092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "iter_num = 0\n",
    "knn_acc2= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    knn_y_hat2 = clf.predict(X2_test)\n",
    "    knn_acc2[iter_num] = mt.accuracy_score(y2_test,knn_y_hat2)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc2[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc2.mean()*100, \"+-\", knn_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.299472457367\n",
      "K Nearest Neighbors  1  accuracy 0.302784934364\n",
      "K Nearest Neighbors  2  accuracy 0.32327321801\n",
      "K Nearest Neighbors  3  accuracy 0.311372837689\n",
      "K Nearest Neighbors  4  accuracy 0.305153374233\n",
      "K Nearest Neighbors  5  accuracy 0.366748466258\n",
      "K Nearest Neighbors  6  accuracy 0.374033623758\n",
      "K Nearest Neighbors  7  accuracy 0.435636274389\n",
      "K Nearest Neighbors  8  accuracy 0.476868327402\n",
      "K Nearest Neighbors  9  accuracy 0.528899251442\n",
      "Average accuracy =  37.2424276491 +- 7.762964895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "iter_num = 0\n",
    "knn_acc2= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    knn_y_hat2 = clf.predict(X2_test)\n",
    "    knn_acc2[iter_num] = mt.accuracy_score(y2_test,knn_y_hat2)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc2[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc2.mean()*100, \"+-\", knn_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.31713900135\n",
      "K Nearest Neighbors  1  accuracy 0.33701386333\n",
      "K Nearest Neighbors  2  accuracy 0.353944301313\n",
      "K Nearest Neighbors  3  accuracy 0.352717457981\n",
      "K Nearest Neighbors  4  accuracy 0.322085889571\n",
      "K Nearest Neighbors  5  accuracy 0.391901840491\n",
      "K Nearest Neighbors  6  accuracy 0.398944655786\n",
      "K Nearest Neighbors  7  accuracy 0.463737881949\n",
      "K Nearest Neighbors  8  accuracy 0.494784636152\n",
      "K Nearest Neighbors  9  accuracy 0.545465701313\n",
      "Average accuracy =  39.7773522923 +- 7.44879925335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "iter_num = 0\n",
    "knn_acc2= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    knn_y_hat2 = clf.predict(X2_test)\n",
    "    knn_acc2[iter_num] = mt.accuracy_score(y2_test,knn_y_hat2)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc2[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc2.mean()*100, \"+-\", knn_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.313458471353\n",
      "K Nearest Neighbors  1  accuracy 0.355293828978\n",
      "K Nearest Neighbors  2  accuracy 0.372346951294\n",
      "K Nearest Neighbors  3  accuracy 0.369402527297\n",
      "K Nearest Neighbors  4  accuracy 0.331901840491\n",
      "K Nearest Neighbors  5  accuracy 0.394969325153\n",
      "K Nearest Neighbors  6  accuracy 0.416492821205\n",
      "K Nearest Neighbors  7  accuracy 0.472327892993\n",
      "K Nearest Neighbors  8  accuracy 0.509019511597\n",
      "K Nearest Neighbors  9  accuracy 0.55061970794\n",
      "Average accuracy =  40.858328783 +- 7.42898382347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=7)\n",
    "iter_num = 0\n",
    "knn_acc2= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    knn_y_hat2 = clf.predict(X2_test)\n",
    "    knn_acc2[iter_num] = mt.accuracy_score(y2_test,knn_y_hat2)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc2[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc2.mean()*100, \"+-\", knn_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Task 1 only (MIleage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression  0  accuracy 0.707888602625\n",
      "Logistic regression  1  accuracy 0.730585204269\n",
      "Logistic regression  2  accuracy 0.752913752914\n",
      "Logistic regression  3  accuracy 0.847258005153\n",
      "Logistic regression  4  accuracy 0.796687116564\n",
      "Logistic regression  5  accuracy 0.76736196319\n",
      "Logistic regression  6  accuracy 0.77248742177\n",
      "Logistic regression  7  accuracy 0.793348877163\n",
      "Logistic regression  8  accuracy 0.788931157197\n",
      "Logistic regression  9  accuracy 0.664744140385\n",
      "Average accuracy =  76.2220624123 +- 4.87592120242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression() # get object\n",
    "iter_num = 0\n",
    "lr_acc= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    lr_y_hat = clf.predict(X_test)\n",
    "    lr_acc[iter_num] = mt.accuracy_score(y_test,lr_y_hat)\n",
    "    print(\"Logistic regression \", iter_num,\" accuracy\", lr_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", lr_acc.mean()*100, \"+-\", lr_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - task 2 only (Body type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Multinomial NB accuracy at alpha=1 is  0.346300159529\n",
      "The Bernoulli NB accuracy at alpha=1 is  0.301263958768\n",
      "The Multinomial NB accuracy at alpha=0.5 is  0.346300159529\n",
      "The Bernoulli NB accuracy at alpha=0.5 is  0.301263958768\n",
      "The Multinomial NB accuracy at alpha=0.1 is  0.346300159529\n",
      "The Bernoulli NB accuracy at alpha=0.1 is  0.301263958768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "clf_mnb = MultinomialNB(alpha=1)\n",
    "y_hat_mnb2 = clf_mnb.fit(X2_train, y2_train).predict(X2_test)\n",
    "acc = mt.accuracy_score(y2_test,y_hat_mnb2)\n",
    "print(\"The Multinomial NB accuracy at alpha=1 is \", acc)\n",
    "\n",
    "clf_bnb = BernoulliNB(alpha=1, binarize=0.0)\n",
    "y_hat_bnb2 = clf_bnb.fit(X2_train, y2_train).predict(X2_test)\n",
    "acc = mt.accuracy_score(y2_test,y_hat_bnb2)\n",
    "print(\"The Bernoulli NB accuracy at alpha=1 is \", acc)\n",
    "\n",
    "clf_mnb = MultinomialNB(alpha=0.5)\n",
    "y_hat_mnb2 = clf_mnb.fit(X2_train, y2_train).predict(X2_test)\n",
    "acc = mt.accuracy_score(y2_test,y_hat_mnb2)\n",
    "print(\"The Multinomial NB accuracy at alpha=0.5 is \", acc)\n",
    "\n",
    "clf_bnb = BernoulliNB(alpha=0.5, binarize=0.0)\n",
    "y_hat_bnb2 = clf_bnb.fit(X2_train, y2_train).predict(X2_test)\n",
    "acc = mt.accuracy_score(y2_test,y_hat_bnb2)\n",
    "print(\"The Bernoulli NB accuracy at alpha=0.5 is \", acc)\n",
    "\n",
    "clf_mnb = MultinomialNB(alpha=0.1)\n",
    "y_hat_mnb2 = clf_mnb.fit(X2_train, y2_train).predict(X2_test)\n",
    "acc = mt.accuracy_score(y2_test,y_hat_mnb2)\n",
    "print(\"The Multinomial NB accuracy at alpha=0.1 is \", acc)\n",
    "\n",
    "clf_bnb = BernoulliNB(alpha=0.1, binarize=0.0)\n",
    "y_hat_bnb2 = clf_bnb.fit(X2_train, y2_train).predict(X2_test)\n",
    "acc = mt.accuracy_score(y2_test,y_hat_bnb2)\n",
    "print(\"The Bernoulli NB accuracy at alpha=0.1 is \", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 4 - Analyze Results\n",
    "\n",
    "Need write up and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 5 - Model advantages\n",
    "\n",
    "Need write up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 6 - Important attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
