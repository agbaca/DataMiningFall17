{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Task 1 Vehicles with over 100K Kilometers/Task 2 Body Type.\n",
    "IN this lab we will be using a dataset found on Kaggle.com about vehcile advertisements in the Czech republic and Germany.  For task 1 we will be focusing on predicting if a vehicle has over 100K kilometers. For task 2 we will be focusing on predicting the different body types of the vehicles.\n",
    "\n",
    "    Scott Gozdzialski\n",
    "    Adam Baca\n",
    "    Zoheb Allam\n",
    "    Ethan Graham\n",
    "    \n",
    "    The data can be found https://www.kaggle.com/mirosval/personal-cars-classifieds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roughly 3.5 Million rows and the following columns:\n",
    "\n",
    "- maker - The manufacturer of the vehicle\n",
    "- model - The distinct model of the vehicle\n",
    "- mileage - in KM (our Response variable)\n",
    "- manufacture_year\n",
    "- engine_displacement - in cc\n",
    "- engine_power - in kW\n",
    "- body_type - Coupe, van, sedan, etc.\n",
    "- color_slug - main color of the vehicle\n",
    "- stk_year - year of the last emission control\n",
    "- transmission - automatic or manual\n",
    "- door_count\n",
    "- seat_count\n",
    "- fuel_type - gasoline, diesel, cng, lpg, electric\n",
    "- date_created - when the ad was scraped\n",
    "- date_last_seen - when the ad was last seen. Our policy was to remove all ads older than 60 days\n",
    "- price_eur - list price converted to EUR\n",
    "\n",
    "The first step is to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (7,8,10,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Import the file of 3.5 Million records we will parse it down to 81000 usable records\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "path = \"~\\\\Desktop\\\\Cars.csv\"\n",
    "\n",
    "df = pd.read_csv(path,sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to have to clean the data.  As can be seen below most of the data is object type wich will not work for our classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3552912 entries, 0 to 3552911\n",
      "Data columns (total 16 columns):\n",
      "maker                  object\n",
      "model                  object\n",
      "mileage                float64\n",
      "manufacture_year       float64\n",
      "engine_displacement    float64\n",
      "engine_power           float64\n",
      "body_type              object\n",
      "color_slug             object\n",
      "stk_year               object\n",
      "transmission           object\n",
      "door_count             object\n",
      "seat_count             object\n",
      "fuel_type              object\n",
      "date_created           object\n",
      "date_last_seen         object\n",
      "price_eur              float64\n",
      "dtypes: float64(5), object(11)\n",
      "memory usage: 433.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we change the date the ad was created and the date it was removed to a interger of the number of days the ad ran. then we drop the columns we will not be using. Stk-year is very close to model year, model takes up to much memory seperate and is unworkable.  Finally we will drop all the rows with NAs.  With 3.5 rows we have plenty to use after removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the date varibles into a delta between and type int\n",
    "df.date_created = pd.to_datetime(df['date_created'])\n",
    "df.date_last_seen = pd.to_datetime(df['date_last_seen'])\n",
    "df['total_days'] = df['date_last_seen'] - df['date_created']\n",
    "df.total_days = df['total_days'].dt.days.astype(int)\n",
    "\n",
    "df.drop(['stk_year','model','date_created','date_last_seen'], axis=1, inplace=True)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert door count and seat count to ints, and remove eronious information.  There are no vehicles with a 10cc engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.door_count = df.door_count.replace('None','0')\n",
    "df.door_count = df.door_count.astype(int)\n",
    "df.seat_count = df.door_count.replace('None','0')\n",
    "df.seat_count = df.door_count.astype(int)\n",
    "\n",
    "df = df.sort_values('engine_displacement', ascending=False)\n",
    "df = df[:82088]\n",
    "\n",
    "df = df.sort_values('engine_power', ascending=False)\n",
    "df = df[:81500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "This is where the dataframe for task one and task two deviate from each other.  Task one will one hot encode body type to predict milage.\n",
    "\n",
    "Then, we OneHotEncode maker, body type,color slug, and fuel type. We turn Transmision to binary (1,0) variable. Finally we remove the columns that we OneHotEncoded.  We also make a back up dataframe incase we make a mistake we can go backto this point without rerunning everything above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df\n",
    "\n",
    "tmp_df = pd.get_dummies(df.maker,prefix='Maker')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.body_type,prefix='Body type')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.color_slug,prefix='Color')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.fuel_type,prefix='Fuel')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "df['manual'] = df.transmission=='man' \n",
    "df.manual = df.manual.astype(np.int)\n",
    "\n",
    "df.drop(['body_type','color_slug','fuel_type','maker','transmission'], axis= 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 81500 entries, 3215366 to 3153910\n",
      "Data columns (total 83 columns):\n",
      "mileage                   81500 non-null float64\n",
      "manufacture_year          81500 non-null float64\n",
      "engine_displacement       81500 non-null float64\n",
      "engine_power              81500 non-null float64\n",
      "door_count                81500 non-null int32\n",
      "seat_count                81500 non-null int32\n",
      "price_eur                 81500 non-null float64\n",
      "total_days                81500 non-null int32\n",
      "Maker_alfa-romeo          81500 non-null uint8\n",
      "Maker_aston-martin        81500 non-null uint8\n",
      "Maker_audi                81500 non-null uint8\n",
      "Maker_bentley             81500 non-null uint8\n",
      "Maker_bmw                 81500 non-null uint8\n",
      "Maker_chevrolet           81500 non-null uint8\n",
      "Maker_chrysler            81500 non-null uint8\n",
      "Maker_citroen             81500 non-null uint8\n",
      "Maker_dacia               81500 non-null uint8\n",
      "Maker_dodge               81500 non-null uint8\n",
      "Maker_fiat                81500 non-null uint8\n",
      "Maker_ford                81500 non-null uint8\n",
      "Maker_honda               81500 non-null uint8\n",
      "Maker_hummer              81500 non-null uint8\n",
      "Maker_hyundai             81500 non-null uint8\n",
      "Maker_infinity            81500 non-null uint8\n",
      "Maker_isuzu               81500 non-null uint8\n",
      "Maker_jaguar              81500 non-null uint8\n",
      "Maker_jeep                81500 non-null uint8\n",
      "Maker_kia                 81500 non-null uint8\n",
      "Maker_lamborghini         81500 non-null uint8\n",
      "Maker_lancia              81500 non-null uint8\n",
      "Maker_land-rover          81500 non-null uint8\n",
      "Maker_lexus               81500 non-null uint8\n",
      "Maker_lotus               81500 non-null uint8\n",
      "Maker_maserati            81500 non-null uint8\n",
      "Maker_mazda               81500 non-null uint8\n",
      "Maker_mercedes-benz       81500 non-null uint8\n",
      "Maker_mini                81500 non-null uint8\n",
      "Maker_mitsubishi          81500 non-null uint8\n",
      "Maker_nissan              81500 non-null uint8\n",
      "Maker_opel                81500 non-null uint8\n",
      "Maker_peugeot             81500 non-null uint8\n",
      "Maker_porsche             81500 non-null uint8\n",
      "Maker_renault             81500 non-null uint8\n",
      "Maker_rolls-royce         81500 non-null uint8\n",
      "Maker_rover               81500 non-null uint8\n",
      "Maker_seat                81500 non-null uint8\n",
      "Maker_skoda               81500 non-null uint8\n",
      "Maker_smart               81500 non-null uint8\n",
      "Maker_subaru              81500 non-null uint8\n",
      "Maker_suzuki              81500 non-null uint8\n",
      "Maker_tesla               81500 non-null uint8\n",
      "Maker_toyota              81500 non-null uint8\n",
      "Maker_volkswagen          81500 non-null uint8\n",
      "Maker_volvo               81500 non-null uint8\n",
      "Body type_compact         81500 non-null uint8\n",
      "Body type_convertible     81500 non-null uint8\n",
      "Body type_coupe           81500 non-null uint8\n",
      "Body type_offroad         81500 non-null uint8\n",
      "Body type_other           81500 non-null uint8\n",
      "Body type_sedan           81500 non-null uint8\n",
      "Body type_stationwagon    81500 non-null uint8\n",
      "Body type_transporter     81500 non-null uint8\n",
      "Body type_van             81500 non-null uint8\n",
      "Color_beige               81500 non-null uint8\n",
      "Color_black               81500 non-null uint8\n",
      "Color_blue                81500 non-null uint8\n",
      "Color_bronze              81500 non-null uint8\n",
      "Color_brown               81500 non-null uint8\n",
      "Color_gold                81500 non-null uint8\n",
      "Color_green               81500 non-null uint8\n",
      "Color_grey                81500 non-null uint8\n",
      "Color_orange              81500 non-null uint8\n",
      "Color_red                 81500 non-null uint8\n",
      "Color_silver              81500 non-null uint8\n",
      "Color_violet              81500 non-null uint8\n",
      "Color_white               81500 non-null uint8\n",
      "Color_yellow              81500 non-null uint8\n",
      "Fuel_cng                  81500 non-null uint8\n",
      "Fuel_diesel               81500 non-null uint8\n",
      "Fuel_electric             81500 non-null uint8\n",
      "Fuel_gasoline             81500 non-null uint8\n",
      "Fuel_lpg                  81500 non-null uint8\n",
      "manual                    81500 non-null int32\n",
      "dtypes: float64(5), int32(4), uint8(74)\n",
      "memory usage: 10.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing mileage to a binary of milage over 100K Kilometers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['mileage_100K'] = df['mileage'] > 100000\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "\n",
    "y = df['mileage_100K'].values # get the labels we want\n",
    "del df['mileage_100K'] \n",
    "del df['mileage']# get rid of the class label\n",
    "X = df.values # use everything else to predict!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Task 2\n",
    "For task 2 we will be predicting the body type and leaving mileage as a consistant number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_df = pd.get_dummies(df2.maker,prefix='Maker')\n",
    "df2 = pd.concat((df2,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df2.color_slug,prefix='Color')\n",
    "df2 = pd.concat((df2,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df2.fuel_type,prefix='Fuel')\n",
    "df2 = pd.concat((df2,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "df2['manual'] = df2.transmission=='man' \n",
    "df2.manual = df2.manual.astype(np.int)\n",
    "\n",
    "df2.drop(['color_slug','fuel_type','maker','transmission'], axis= 1, inplace = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2 = df2['body_type'].values # get the labels we want\n",
    "del df2['body_type'] \n",
    "X2 = df2.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation part 2 - Final dataset.\n",
    "The final dataset constists of 81500 records with 82 columns.\n",
    "\n",
    "First we downloaded our dataset of car sales in the Czech Republic and Germany.  Most of it downloads as obect type so we changed door count and seat count to intergers.  \n",
    "\n",
    "We calculated the difference between when the advertisment started and was dropped and created a new variable of total days the ad ran.    \n",
    "\n",
    "We dropped the stk-year and model, stk-year is not needed since it is very similar to model year.  Model has to many classifications in the rows for us to be able to seperate it out.  When we tried we ran out of memory.\n",
    "\n",
    "#### Task 1\n",
    "\n",
    "Speaking of seperation the classifactions out, we spereated out Maker, Body type, Color, and Fuel type with one hot endoing. \n",
    "\n",
    "#### Task 2\n",
    "\n",
    "For the seperations in task 2, we seperated out Maker, Color, and Fuel Type with one hot encoding.  We left body type in the dataframe because this will now be our response variable.  Body type has 9 different classifaction we will be focusing on predicting.\n",
    "\n",
    "#### Both tasks\n",
    "We also, dropped any row with a NA value.  This was done for two reasons, first it removed useless rows that will mess with our classifiaction models, second since we started with 3.5M records dropping the rows with NAs left us with 81500 rows of usable data.  The entire 3.5M records would eat up the resources of our machines and 81500 records should be a large enough sampleset to properly capture the nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model and Evaluation 1 - Evaluation Metric.\n",
    "\n",
    "To evaluate our different classification methods, we will be examining the accuracy with which they can predict vehicles with over 100k miles (task 1) and vehicle body type (task 2). The accuracy of each method will tell us the percentage of our sample correctly classified (PCC), in otherwords the percent of true positives and true negatives. PCC is the most commonly used metric to assess overall model accuracy and is calculated without taking into account what kind of errors are made, meaning each error has the same weight. Since our models aim to predict features in cars and not something related to health care like cancer, we are not concerned about the different impacts the false positives and false negatives may have on our sample and therefore can be treated equally.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Evaluation 2 - dividing data\n",
    "\n",
    "We will be using 10 fold cross validation in order to divide our data into training and testing splits. Cross validation is when you divide a sample of data into subsets and then perform the analysis on a training subset and validate those results with the testing subset. Cross validation allows you to determine if the results of the model will generalize to an independent dataset and also can limit issues like overfitting. \n",
    "\n",
    "With 10 fold cross validation, the cross validation process is repeated 10 times with each of the subsamples being used only once for validation. The main advantages to repeating this process 10 times is that all observations are used for both training and validation and therefore we do not lose sample size which can affect modeling capabilities. \n",
    "\n",
    "In order to aggregate the results from the 10 fold cross validation we will take the PCC for each model and average it together to form an overall accuracy measure. This will enable us to get a more accurate estimate of each models performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Evaluation 3 - Model selection\n",
    "\n",
    "This is the section where we run different models and evaluate the results.  Each model will be run more than once with different settings.  \n",
    "\n",
    "For task 1 (mileage) we will  run a random forest, a k nearest neighbors, and a logistic regression. \n",
    "\n",
    "For task 2  (body type) we will run a random forest, a k nearest neightbors, and adaboost ensemble.\n",
    "\n",
    "When we are done.  We will place the models that run the best of each type, for a total of 6, together for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - task 1 (mileage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a randdom forest with 50 classifers as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.428045638572\n",
      "Random Forest  1  accuracy 0.855109802478\n",
      "Random Forest  2  accuracy 0.87314439946\n",
      "Random Forest  3  accuracy 0.84713532082\n",
      "Random Forest  4  accuracy 0.847116564417\n",
      "Random Forest  5  accuracy 0.794601226994\n",
      "Random Forest  6  accuracy 0.772610136213\n",
      "Random Forest  7  accuracy 0.744263099767\n",
      "Random Forest  8  accuracy 0.801816173764\n",
      "Random Forest  9  accuracy 0.746717388637\n",
      "Average accuracy =  77.1055975112 +- 12.2282325962\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50,random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    rf_y_hat = clf.predict(X_test)\n",
    "    rf_acc[iter_num] = mt.accuracy_score(y_test,rf_y_hat)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc.mean()*100, \"+-\", rf_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, trying 10 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.401177769599\n",
      "Random Forest  1  accuracy 0.836093730831\n",
      "Random Forest  2  accuracy 0.867746288799\n",
      "Random Forest  3  accuracy 0.840387682493\n",
      "Random Forest  4  accuracy 0.834969325153\n",
      "Random Forest  5  accuracy 0.767484662577\n",
      "Random Forest  6  accuracy 0.76230212296\n",
      "Random Forest  7  accuracy 0.739599950914\n",
      "Random Forest  8  accuracy 0.778377715057\n",
      "Random Forest  9  accuracy 0.709166768929\n",
      "Average accuracy =  75.3730601731 +- 12.6980973793\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "iter_num = 0\n",
    "rf_acc = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    rf_y_hat = clf.predict(X_test)\n",
    "    rf_acc[iter_num] = mt.accuracy_score(y_test,rf_y_hat)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc.mean()*100, \"+-\", rf_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 did worse, so trying 100 to see if there is an improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.448411237885\n",
      "Random Forest  1  accuracy 0.855723224144\n",
      "Random Forest  2  accuracy 0.873021715127\n",
      "Random Forest  3  accuracy 0.852165378481\n",
      "Random Forest  4  accuracy 0.848220858896\n",
      "Random Forest  5  accuracy 0.79963190184\n",
      "Random Forest  6  accuracy 0.774573567309\n",
      "Random Forest  7  accuracy 0.744263099767\n",
      "Random Forest  8  accuracy 0.814333047\n",
      "Random Forest  9  accuracy 0.749048963063\n",
      "Average accuracy =  77.5939299351 +- 11.7415457435\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "iter_num = 0\n",
    "rf_acc = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    rf_y_hat = clf.predict(X_test)\n",
    "    rf_acc[iter_num] = mt.accuracy_score(y_test,rf_y_hat)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc.mean()*100, \"+-\", rf_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no difference between 50 and 100 estimators.  Will use 50 estimators and adjust the max depth.  Starting at 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.540547172126\n",
      "Random Forest  1  accuracy 0.858054226475\n",
      "Random Forest  2  accuracy 0.868114341799\n",
      "Random Forest  3  accuracy 0.866519445467\n",
      "Random Forest  4  accuracy 0.872883435583\n",
      "Random Forest  5  accuracy 0.86981595092\n",
      "Random Forest  6  accuracy 0.812124187017\n",
      "Random Forest  7  accuracy 0.877408270953\n",
      "Random Forest  8  accuracy 0.854460670021\n",
      "Random Forest  9  accuracy 0.741440667567\n",
      "Average accuracy =  81.6136836793 +- 9.99334512427\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50,max_depth=10,random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    rf_y_hat = clf.predict(X_test)\n",
    "    rf_acc[iter_num] = mt.accuracy_score(y_test,rf_y_hat)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc.mean()*100, \"+-\", rf_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max depth of ten is an imporvement.  Will continue to adjust the max depth.  Trying 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.790332474543\n",
      "Random Forest  1  accuracy 0.816464237517\n",
      "Random Forest  2  accuracy 0.832045147835\n",
      "Random Forest  3  accuracy 0.827505827506\n",
      "Random Forest  4  accuracy 0.865889570552\n",
      "Random Forest  5  accuracy 0.88\n",
      "Random Forest  6  accuracy 0.786967726101\n",
      "Random Forest  7  accuracy 0.883543993128\n",
      "Random Forest  8  accuracy 0.841575653454\n",
      "Random Forest  9  accuracy 0.735059516505\n",
      "Average accuracy =  82.5938414714 +- 4.3882407008\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50,max_depth=5,random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    rf_y_hat = clf.predict(X_test)\n",
    "    rf_acc[iter_num] = mt.accuracy_score(y_test,rf_y_hat)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc.mean()*100, \"+-\", rf_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max depth 5 is an increase of accuracy mena and a reduction of standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.861366703472\n",
      "Random Forest  1  accuracy 0.741994847258\n",
      "Random Forest  2  accuracy 0.763341921237\n",
      "Random Forest  3  accuracy 0.787878787879\n",
      "Random Forest  4  accuracy 0.788343558282\n",
      "Random Forest  5  accuracy 0.870306748466\n",
      "Random Forest  6  accuracy 0.759234261873\n",
      "Random Forest  7  accuracy 0.868572831022\n",
      "Random Forest  8  accuracy 0.789422014971\n",
      "Random Forest  9  accuracy 0.708430482268\n",
      "Average accuracy =  79.3889215673 +- 5.30920447228\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50,max_depth=3,random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    rf_y_hat = clf.predict(X_test)\n",
    "    rf_acc[iter_num] = mt.accuracy_score(y_test,rf_y_hat)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc.mean()*100, \"+-\", rf_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max depth of 3 decreases the mean accuracy and increases the standard deviation.  \n",
    "\n",
    "We will stick with number of estimators at 50 and tree depth of 5 for mileage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - task 2 (Body type)\n",
    "\n",
    "Starting with a randdom forest with 50 classifers as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.587289903079\n",
      "Random Forest  1  accuracy 0.600785179733\n",
      "Random Forest  2  accuracy 0.596736596737\n",
      "Random Forest  3  accuracy 0.595632437738\n",
      "Random Forest  4  accuracy 0.56282208589\n",
      "Random Forest  5  accuracy 0.576196319018\n",
      "Random Forest  6  accuracy 0.571481163333\n",
      "Random Forest  7  accuracy 0.581298318812\n",
      "Random Forest  8  accuracy 0.656644987115\n",
      "Random Forest  9  accuracy 0.692232175727\n",
      "Average accuracy =  60.2111916718 +- 3.86970345172\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50,random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc2 = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    rf_y_hat2 = clf.predict(X2_test)\n",
    "    rf_acc2[iter_num] = mt.accuracy_score(y2_test,rf_y_hat2)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc2[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc2.mean()*100, \"+-\", rf_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.599681020734\n",
      "Random Forest  1  accuracy 0.601766654398\n",
      "Random Forest  2  accuracy 0.5999263894\n",
      "Random Forest  3  accuracy 0.603852288063\n",
      "Random Forest  4  accuracy 0.564417177914\n",
      "Random Forest  5  accuracy 0.581226993865\n",
      "Random Forest  6  accuracy 0.575285311081\n",
      "Random Forest  7  accuracy 0.584120751012\n",
      "Random Forest  8  accuracy 0.662657994846\n",
      "Random Forest  9  accuracy 0.691127745736\n",
      "Average accuracy =  60.6406232705 +- 3.78411018964\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc2 = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    rf_y_hat2 = clf.predict(X2_test)\n",
    "    rf_acc2[iter_num] = mt.accuracy_score(y2_test,rf_y_hat2)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc2[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc2.mean()*100, \"+-\", rf_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost no difference when adjusting number of estimators.  Moving on to max depth.  Starting with a max depth of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.536743957797\n",
      "Random Forest  1  accuracy 0.60642865906\n",
      "Random Forest  2  accuracy 0.585940375414\n",
      "Random Forest  3  accuracy 0.54643602012\n",
      "Random Forest  4  accuracy 0.449570552147\n",
      "Random Forest  5  accuracy 0.534355828221\n",
      "Random Forest  6  accuracy 0.551233280157\n",
      "Random Forest  7  accuracy 0.597619339796\n",
      "Random Forest  8  accuracy 0.642900969444\n",
      "Random Forest  9  accuracy 0.668548288134\n",
      "Average accuracy =  57.1977727029 +- 5.92607351071\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,max_depth=10, random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc2 = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    rf_y_hat2 = clf.predict(X2_test)\n",
    "    rf_acc2[iter_num] = mt.accuracy_score(y2_test,rf_y_hat2)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc2[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc2.mean()*100, \"+-\", rf_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the max depth seems to make the model worse.  Will try a depth of 5 to see if there is any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  0  accuracy 0.47343884186\n",
      "Random Forest  1  accuracy 0.536743957797\n",
      "Random Forest  2  accuracy 0.540792540793\n",
      "Random Forest  3  accuracy 0.502146975831\n",
      "Random Forest  4  accuracy 0.40036809816\n",
      "Random Forest  5  accuracy 0.492883435583\n",
      "Random Forest  6  accuracy 0.528285679224\n",
      "Random Forest  7  accuracy 0.562154865628\n",
      "Random Forest  8  accuracy 0.577248742177\n",
      "Random Forest  9  accuracy 0.638851392809\n",
      "Average accuracy =  52.5291452986 +- 6.10042820473\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,max_depth=5, random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc2 = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    rf_y_hat2 = clf.predict(X2_test)\n",
    "    rf_acc2[iter_num] = mt.accuracy_score(y2_test,rf_y_hat2)\n",
    "    print(\"Random Forest \", iter_num,\" accuracy\", rf_acc2[iter_num])\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Average accuracy = \", rf_acc2.mean()*100, \"+-\", rf_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the max depth makes the randomforest worse for the classification of body type.  We will stick to having the computer pick the max depth. \n",
    "\n",
    "The best random forest for body type is 100 estimators and the computer picking the depth,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors - Task 1 (Mileage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test 1,3,5, and 7 neighbors\n",
    "\n",
    "Starting with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.505091399828\n",
      "K Nearest Neighbors  1  accuracy 0.696724328303\n",
      "K Nearest Neighbors  2  accuracy 0.760152128573\n",
      "K Nearest Neighbors  3  accuracy 0.700650226966\n",
      "K Nearest Neighbors  4  accuracy 0.665521472393\n",
      "K Nearest Neighbors  5  accuracy 0.674969325153\n",
      "K Nearest Neighbors  6  accuracy 0.661798993742\n",
      "K Nearest Neighbors  7  accuracy 0.653945269358\n",
      "K Nearest Neighbors  8  accuracy 0.693827463492\n",
      "K Nearest Neighbors  9  accuracy 0.6842557369\n",
      "Average accuracy =  66.9693634471 +- 6.17378948196\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "iter_num = 0\n",
    "knn_acc= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    knn_y_hat = clf.predict(X_test)\n",
    "    knn_acc[iter_num] = mt.accuracy_score(y_test,knn_y_hat)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc.mean()*100, \"+-\", knn_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knn of 1 has ok results will try 3 neighbors next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.521899153478\n",
      "K Nearest Neighbors  1  accuracy 0.714513556619\n",
      "K Nearest Neighbors  2  accuracy 0.770580296896\n",
      "K Nearest Neighbors  3  accuracy 0.719175561281\n",
      "K Nearest Neighbors  4  accuracy 0.694969325153\n",
      "K Nearest Neighbors  5  accuracy 0.699386503067\n",
      "K Nearest Neighbors  6  accuracy 0.688428027979\n",
      "K Nearest Neighbors  7  accuracy 0.684501165787\n",
      "K Nearest Neighbors  8  accuracy 0.7229107866\n",
      "K Nearest Neighbors  9  accuracy 0.686219167996\n",
      "Average accuracy =  69.0258354486 +- 6.11926978358\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "iter_num = 0\n",
    "knn_acc= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    knn_y_hat = clf.predict(X_test)\n",
    "    knn_acc[iter_num] = mt.accuracy_score(y_test,knn_y_hat)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc.mean()*100, \"+-\", knn_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 neightbors has an increase in mean average, but the standard deviation has not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.525456999141\n",
      "K Nearest Neighbors  1  accuracy 0.720525088946\n",
      "K Nearest Neighbors  2  accuracy 0.770212243896\n",
      "K Nearest Neighbors  3  accuracy 0.724328303276\n",
      "K Nearest Neighbors  4  accuracy 0.71509202454\n",
      "K Nearest Neighbors  5  accuracy 0.70981595092\n",
      "K Nearest Neighbors  6  accuracy 0.696649895693\n",
      "K Nearest Neighbors  7  accuracy 0.69480917904\n",
      "K Nearest Neighbors  8  accuracy 0.724628788808\n",
      "K Nearest Neighbors  9  accuracy 0.70217204565\n",
      "Average accuracy =  69.8369051991 +- 6.11188587778\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "iter_num = 0\n",
    "knn_acc= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    knn_y_hat = clf.predict(X_test)\n",
    "    knn_acc[iter_num] = mt.accuracy_score(y_test,knn_y_hat)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc.mean()*100, \"+-\", knn_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is almost no difference between 3 and 5 neighbors.  We will try 7 neighbors to see what happens, my guess before we run 7 neighbors is that the model will have a slight decrease in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.530609741136\n",
      "K Nearest Neighbors  1  accuracy 0.722856091277\n",
      "K Nearest Neighbors  2  accuracy 0.765918292234\n",
      "K Nearest Neighbors  3  accuracy 0.737332842596\n",
      "K Nearest Neighbors  4  accuracy 0.730552147239\n",
      "K Nearest Neighbors  5  accuracy 0.729325153374\n",
      "K Nearest Neighbors  6  accuracy 0.695422751258\n",
      "K Nearest Neighbors  7  accuracy 0.703644618972\n",
      "K Nearest Neighbors  8  accuracy 0.734077800957\n",
      "K Nearest Neighbors  9  accuracy 0.706957908946\n",
      "Average accuracy =  70.5669734799 +- 6.137614028\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=7)\n",
    "iter_num = 0\n",
    "knn_acc= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    knn_y_hat = clf.predict(X_test)\n",
    "    knn_acc[iter_num] = mt.accuracy_score(y_test,knn_y_hat)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc.mean()*100, \"+-\", knn_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly the KNN with 7 neighbors has the best mean accuracy and the standard deviation is almost exactly the same.\n",
    "\n",
    "The best KNN model for mileage is 7 neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neightbors Task 2 ( Body Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test 1,3,5, and 7 neighbors\n",
    "\n",
    "Staring with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.267574530732\n",
      "K Nearest Neighbors  1  accuracy 0.302416881364\n",
      "K Nearest Neighbors  2  accuracy 0.324745430009\n",
      "K Nearest Neighbors  3  accuracy 0.312722365354\n",
      "K Nearest Neighbors  4  accuracy 0.306871165644\n",
      "K Nearest Neighbors  5  accuracy 0.350552147239\n",
      "K Nearest Neighbors  6  accuracy 0.351454166155\n",
      "K Nearest Neighbors  7  accuracy 0.395385936925\n",
      "K Nearest Neighbors  8  accuracy 0.430114124432\n",
      "K Nearest Neighbors  9  accuracy 0.45465701313\n",
      "Average accuracy =  34.9649376099 +- 5.69017219092\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "iter_num = 0\n",
    "knn_acc2= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    knn_y_hat2 = clf.predict(X2_test)\n",
    "    knn_acc2[iter_num] = mt.accuracy_score(y2_test,knn_y_hat2)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc2[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc2.mean()*100, \"+-\", knn_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 neighbor does not seem that srtong, so we will adjust to 3 neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.299472457367\n",
      "K Nearest Neighbors  1  accuracy 0.302784934364\n",
      "K Nearest Neighbors  2  accuracy 0.32327321801\n",
      "K Nearest Neighbors  3  accuracy 0.311372837689\n",
      "K Nearest Neighbors  4  accuracy 0.305153374233\n",
      "K Nearest Neighbors  5  accuracy 0.366748466258\n",
      "K Nearest Neighbors  6  accuracy 0.374033623758\n",
      "K Nearest Neighbors  7  accuracy 0.435636274389\n",
      "K Nearest Neighbors  8  accuracy 0.476868327402\n",
      "K Nearest Neighbors  9  accuracy 0.528899251442\n",
      "Average accuracy =  37.2424276491 +- 7.762964895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "iter_num = 0\n",
    "knn_acc2= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    knn_y_hat2 = clf.predict(X2_test)\n",
    "    knn_acc2[iter_num] = mt.accuracy_score(y2_test,knn_y_hat2)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc2[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc2.mean()*100, \"+-\", knn_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 neighbors has a greater accuacy mean, but has a greatere standard deviation.  Next is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.31713900135\n",
      "K Nearest Neighbors  1  accuracy 0.33701386333\n",
      "K Nearest Neighbors  2  accuracy 0.353944301313\n",
      "K Nearest Neighbors  3  accuracy 0.352717457981\n",
      "K Nearest Neighbors  4  accuracy 0.322085889571\n",
      "K Nearest Neighbors  5  accuracy 0.391901840491\n",
      "K Nearest Neighbors  6  accuracy 0.398944655786\n",
      "K Nearest Neighbors  7  accuracy 0.463737881949\n",
      "K Nearest Neighbors  8  accuracy 0.494784636152\n",
      "K Nearest Neighbors  9  accuracy 0.545465701313\n",
      "Average accuracy =  39.7773522923 +- 7.44879925335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "iter_num = 0\n",
    "knn_acc2= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    knn_y_hat2 = clf.predict(X2_test)\n",
    "    knn_acc2[iter_num] = mt.accuracy_score(y2_test,knn_y_hat2)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc2[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc2.mean()*100, \"+-\", knn_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 neighbors has a greater accuacy mean, but has no change to standard deviation.  Next is 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors  0  accuracy 0.313458471353\n",
      "K Nearest Neighbors  1  accuracy 0.355293828978\n",
      "K Nearest Neighbors  2  accuracy 0.372346951294\n",
      "K Nearest Neighbors  3  accuracy 0.369402527297\n",
      "K Nearest Neighbors  4  accuracy 0.331901840491\n",
      "K Nearest Neighbors  5  accuracy 0.394969325153\n",
      "K Nearest Neighbors  6  accuracy 0.416492821205\n",
      "K Nearest Neighbors  7  accuracy 0.472327892993\n",
      "K Nearest Neighbors  8  accuracy 0.509019511597\n",
      "K Nearest Neighbors  9  accuracy 0.55061970794\n",
      "Average accuracy =  40.858328783 +- 7.42898382347\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=7)\n",
    "iter_num = 0\n",
    "knn_acc2= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    knn_y_hat2 = clf.predict(X2_test)\n",
    "    knn_acc2[iter_num] = mt.accuracy_score(y2_test,knn_y_hat2)\n",
    "    print(\"K Nearest Neighbors \", iter_num,\" accuracy\", knn_acc2[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", knn_acc2.mean()*100, \"+-\", knn_acc2.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 has the best accuaracy so far, but the standard devation of KNN 1 is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Task 1 only (MIleage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression  0  accuracy 0.707888602625\n",
      "Logistic regression  1  accuracy 0.730585204269\n",
      "Logistic regression  2  accuracy 0.752913752914\n",
      "Logistic regression  3  accuracy 0.847258005153\n",
      "Logistic regression  4  accuracy 0.796687116564\n",
      "Logistic regression  5  accuracy 0.76736196319\n",
      "Logistic regression  6  accuracy 0.77248742177\n",
      "Logistic regression  7  accuracy 0.793348877163\n",
      "Logistic regression  8  accuracy 0.788931157197\n",
      "Logistic regression  9  accuracy 0.664744140385\n",
      "Average accuracy =  76.2220624123 +- 4.87592120242\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression() # get object\n",
    "iter_num = 0\n",
    "lr_acc= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    lr_y_hat = clf.predict(X_test)\n",
    "    lr_acc[iter_num] = mt.accuracy_score(y_test,lr_y_hat)\n",
    "    print(\"Logistic regression \", iter_num,\" accuracy\", lr_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", lr_acc.mean()*100, \"+-\", lr_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the logistic regression with a l1 penalty.  ( l2 is the default penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression  0  accuracy 0.715004293952\n",
      "Logistic regression  1  accuracy 0.755367439578\n",
      "Logistic regression  2  accuracy 0.768985400564\n",
      "Logistic regression  3  accuracy 0.863452337137\n",
      "Logistic regression  4  accuracy 0.82490797546\n",
      "Logistic regression  5  accuracy 0.781963190184\n",
      "Logistic regression  6  accuracy 0.789544729415\n",
      "Logistic regression  7  accuracy 0.830899496871\n",
      "Logistic regression  8  accuracy 0.806111179286\n",
      "Logistic regression  9  accuracy 0.702785617867\n",
      "Average accuracy =  78.3902166031 +- 4.80737405694\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty ='l1') # get object\n",
    "iter_num = 0\n",
    "lr_acc= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    clf.fit(X_train,y_train) \n",
    "    lr_y_hat = clf.predict(X_test)\n",
    "    lr_acc[iter_num] = mt.accuracy_score(y_test,lr_y_hat)\n",
    "    print(\"Logistic regression \", iter_num,\" accuracy\", lr_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", lr_acc.mean()*100, \"+-\", lr_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a l1 penalty there is an increase in mean accuarcy and no change in standard deviation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost - task 2 only (Body type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final model of task 2 we will using Adaboost.  We will be adjust the n_estimators starting at 50 and trying 100 afterwards.  Then we will tray changing the learning rate.  The default learning rate is 1.  We will pick the best number of estimators and also try 1.5 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost  0  accuracy 0.208686050791\n",
      "Adaboost  1  accuracy 0.41307814992\n",
      "Adaboost  2  accuracy 0.452950558214\n",
      "Adaboost  3  accuracy 0.420193841246\n",
      "Adaboost  4  accuracy 0.332392638037\n",
      "Adaboost  5  accuracy 0.452147239264\n",
      "Adaboost  6  accuracy 0.497975211682\n",
      "Adaboost  7  accuracy 0.516627807093\n",
      "Adaboost  8  accuracy 0.319916554178\n",
      "Adaboost  9  accuracy 0.298196097681\n",
      "Average accuracy =  39.1216414811 +- 9.29950399682\n"
     ]
    }
   ],
   "source": [
    "iter_num = 0\n",
    "ad_acc= np.zeros(10)\n",
    "\n",
    "iter_num = 0\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y):\n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=True)\n",
    "    clf = AdaBoostClassifier(n_estimators=50, random_state=True)\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    ad_y2_hat = clf.predict(X2_test)\n",
    "    ad_acc[iter_num] = mt.accuracy_score(y2_test,ad_y2_hat)\n",
    "    print(\"Adaboost \", iter_num,\" accuracy\", ad_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", ad_acc.mean()*100, \"+-\", ad_acc.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost  0  accuracy 0.208686050791\n",
      "Adaboost  1  accuracy 0.41307814992\n",
      "Adaboost  2  accuracy 0.452950558214\n",
      "Adaboost  3  accuracy 0.420193841246\n",
      "Adaboost  4  accuracy 0.332392638037\n",
      "Adaboost  5  accuracy 0.452147239264\n",
      "Adaboost  6  accuracy 0.497975211682\n",
      "Adaboost  7  accuracy 0.516627807093\n",
      "Adaboost  8  accuracy 0.319916554178\n",
      "Adaboost  9  accuracy 0.298196097681\n",
      "Average accuracy =  39.1216414811 +- 9.29950399682\n"
     ]
    }
   ],
   "source": [
    "iter_num = 0\n",
    "ad_acc= np.zeros(10)\n",
    "\n",
    "iter_num = 0\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y):\n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=True)\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=True)\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    ad_y2_hat = clf.predict(X2_test)\n",
    "    ad_acc[iter_num] = mt.accuracy_score(y2_test,ad_y2_hat)\n",
    "    print(\"Adaboost \", iter_num,\" accuracy\", ad_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", ad_acc.mean()*100, \"+-\", ad_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are exactly the same based upon the number of estimators bewteen 50 and 100.  we will try dropping it to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost  0  accuracy 0.20979020979\n",
      "Adaboost  1  accuracy 0.41307814992\n",
      "Adaboost  2  accuracy 0.452827873881\n",
      "Adaboost  3  accuracy 0.420071156913\n",
      "Adaboost  4  accuracy 0.332024539877\n",
      "Adaboost  5  accuracy 0.451779141104\n",
      "Adaboost  6  accuracy 0.498097926126\n",
      "Adaboost  7  accuracy 0.515768805988\n",
      "Adaboost  8  accuracy 0.304945392073\n",
      "Adaboost  9  accuracy 0.277948214505\n",
      "Average accuracy =  38.7633141018 +- 9.6059577814\n"
     ]
    }
   ],
   "source": [
    "iter_num = 0\n",
    "ad_acc= np.zeros(10)\n",
    "\n",
    "iter_num = 0\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y):\n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=True)\n",
    "    clf = AdaBoostClassifier(n_estimators=25, random_state=True)\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    ad_y2_hat = clf.predict(X2_test)\n",
    "    ad_acc[iter_num] = mt.accuracy_score(y2_test,ad_y2_hat)\n",
    "    print(\"Adaboost \", iter_num,\" accuracy\", ad_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", ad_acc.mean()*100, \"+-\", ad_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 dropped the accuracy a little but there is not much to give so we will stick with 50 for the next step.\n",
    "\n",
    "Now we will adjust the learning rate from 1 (default) to 1.5 and 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost  0  accuracy 0.358483621642\n",
      "Adaboost  1  accuracy 0.432094221568\n",
      "Adaboost  2  accuracy 0.509630720157\n",
      "Adaboost  3  accuracy 0.45626303521\n",
      "Adaboost  4  accuracy 0.387852760736\n",
      "Adaboost  5  accuracy 0.410429447853\n",
      "Adaboost  6  accuracy 0.455025156461\n",
      "Adaboost  7  accuracy 0.449503006504\n",
      "Adaboost  8  accuracy 0.47294146521\n",
      "Adaboost  9  accuracy 0.668302859247\n",
      "Average accuracy =  46.0052629459 +- 8.05707239274\n"
     ]
    }
   ],
   "source": [
    "iter_num = 0\n",
    "ad_acc= np.zeros(10)\n",
    "\n",
    "iter_num = 0\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y):\n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=True)\n",
    "    clf = AdaBoostClassifier(n_estimators= 50,learning_rate = 1.5, random_state=True)\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    ad_y2_hat = clf.predict(X2_test)\n",
    "    ad_acc[iter_num] = mt.accuracy_score(y2_test,ad_y2_hat)\n",
    "    print(\"Adaboost \", iter_num,\" accuracy\", ad_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", ad_acc.mean()*100, \"+-\", ad_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a huge inprovement in predictablity from 395 to 46%. The standard deviation also improved.  \n",
    "\n",
    "Now to try learning_rate of 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost  0  accuracy 0.350754508649\n",
      "Adaboost  1  accuracy 0.381548276285\n",
      "Adaboost  2  accuracy 0.408906882591\n",
      "Adaboost  3  accuracy 0.278248067722\n",
      "Adaboost  4  accuracy 0.298895705521\n",
      "Adaboost  5  accuracy 0.363558282209\n",
      "Adaboost  6  accuracy 0.429377837772\n",
      "Adaboost  7  accuracy 0.491962203951\n",
      "Adaboost  8  accuracy 0.497238925021\n",
      "Adaboost  9  accuracy 0.573567308872\n",
      "Average accuracy =  40.7405799859 +- 8.80745094199\n"
     ]
    }
   ],
   "source": [
    "iter_num = 0\n",
    "ad_acc= np.zeros(10)\n",
    "\n",
    "iter_num = 0\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y):\n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=True)\n",
    "    clf = AdaBoostClassifier(n_estimators= 50,learning_rate = 2, random_state=True)\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    ad_y2_hat = clf.predict(X2_test)\n",
    "    ad_acc[iter_num] = mt.accuracy_score(y2_test,ad_y2_hat)\n",
    "    print(\"Adaboost \", iter_num,\" accuracy\", ad_acc[iter_num] )\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Average accuracy = \", ad_acc.mean()*100, \"+-\", ad_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the learning rate to 2.0 has dropped the accuracy so we will stick with 50 estimators nad a learning rate of 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So here is the best models for each task and model type.\n",
    "\n",
    "*******************************************************\n",
    "\n",
    "Random Forest - 50 estimators with a max depth of 5\n",
    "\n",
    "For task 1\n",
    "\n",
    "KNN - neighbors 7\n",
    "\n",
    "Logisitc Regression - l1 penalty\n",
    "\n",
    "*******************************************************\n",
    "For task 2\n",
    "\n",
    "random Forest - 100 estimators with the computer picking the depth\n",
    "\n",
    "KNN - neighbors 7\n",
    "\n",
    "Adaboost - 50 estimators and a learning rate of 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taks 1 Random Forest Average accuracy =  82.5938414714 +- 4.3882407008\n",
      "Task 2 Random Forest Average accuracy =  60.6406232705 +- 3.78411018964\n",
      "Task 1 KNN Average accuracy =  70.5669734799 +- 6.137614028\n",
      "taks 2 KNN Average accuracy =  40.858328783 +- 7.42898382347\n",
      "Task 1 Logistic Regression Average accuracy =  78.3828365742 +- 4.66070079553\n",
      "Task 2 Adaboost Average accuracy =  46.0052629459 +- 8.05707239274\n"
     ]
    }
   ],
   "source": [
    "#*************************************Task 2 Random Forest\n",
    "rf1clf = RandomForestClassifier(n_estimators=50,max_depth=5,random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    rf1clf.fit(X_train,y_train) \n",
    "    rf_y_hat = rf1clf.predict(X_test)\n",
    "    rf_acc[iter_num] = mt.accuracy_score(y_test,rf_y_hat)\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Taks 1 Random Forest Average accuracy = \", rf_acc.mean()*100, \"+-\", rf_acc.std()*100)\n",
    "\n",
    "#************************************Task 2 Random forest\n",
    "rf2clf = RandomForestClassifier(n_estimators=100,random_state=1)\n",
    "iter_num = 0\n",
    "rf_acc2 = np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    rf2clf.fit(X2_train,y2_train) \n",
    "    rf_y_hat2 = rf2clf.predict(X2_test)\n",
    "    rf_acc2[iter_num] = mt.accuracy_score(y2_test,rf_y_hat2)\n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print (\"Task 2 Random Forest Average accuracy = \", rf_acc2.mean()*100, \"+-\", rf_acc2.std()*100)\n",
    "\n",
    "#************************************Taks 1 KNN\n",
    "k1clf = KNeighborsClassifier(n_neighbors=7)\n",
    "iter_num = 0\n",
    "knn_acc= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    k1clf.fit(X_train,y_train) \n",
    "    knn_y_hat = k1clf.predict(X_test)\n",
    "    knn_acc[iter_num] = mt.accuracy_score(y_test,knn_y_hat)\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Task 1 KNN Average accuracy = \", knn_acc.mean()*100, \"+-\", knn_acc.std()*100)\n",
    "\n",
    "#************************************Task 2 KNN\n",
    "k2clf = KNeighborsClassifier(n_neighbors=7)\n",
    "iter_num = 0\n",
    "knn_acc2= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    k2clf.fit(X2_train,y2_train) \n",
    "    knn_y_hat2 = k2clf.predict(X2_test)\n",
    "    knn_acc2[iter_num] = mt.accuracy_score(y2_test,knn_y_hat2)\n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"taks 2 KNN Average accuracy = \", knn_acc2.mean()*100, \"+-\", knn_acc2.std()*100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#************************************* task 1 logistic regression\n",
    "lrclf = LogisticRegression(penalty ='l1') \n",
    "iter_num = 0\n",
    "lr_acc= np.zeros(10)\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    lrclf.fit(X_train,y_train) \n",
    "    lr_y_hat = lrclf.predict(X_test)\n",
    "    lr_acc[iter_num] = mt.accuracy_score(y_test,lr_y_hat)\n",
    "    \n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Task 1 Logistic Regression Average accuracy = \", lr_acc.mean()*100, \"+-\", lr_acc.std()*100)\n",
    "\n",
    "\n",
    "#*************************************Task 2 adaboost\n",
    "iter_num = 0\n",
    "ad_acc= np.zeros(10)\n",
    "\n",
    "iter_num = 0\n",
    "\n",
    "for train_indices, test_indices in cv.split(X,y):\n",
    "    X2_train = X2[train_indices]\n",
    "    y2_train = y2[train_indices]\n",
    "    X2_test = X2[test_indices]\n",
    "    y2_test = y2[test_indices]\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=True)\n",
    "    clf = AdaBoostClassifier(n_estimators= 50,learning_rate = 1.5, random_state=True)\n",
    "    clf.fit(X2_train,y2_train) \n",
    "    ad_y2_hat = clf.predict(X2_test)\n",
    "    ad_acc[iter_num] = mt.accuracy_score(y2_test,ad_y2_hat)\n",
    "    \n",
    "    iter_num+=1\n",
    "    \n",
    "print (\"Task 2 Adaboost Average accuracy = \", ad_acc.mean()*100, \"+-\", ad_acc.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 4 - Analyze Results\n",
    "\n",
    "Need write up and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start by just plotting what we previsously grouped!\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "df_grouped = df.groupby(by=['Pclass','age_range'])\n",
    "mileage_rate = df_grouped.Survived.sum() / df_grouped.Survived.count()\n",
    "ax = mileage_rate.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 5 - Model advantages\n",
    "\n",
    "Need write up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 6 - Important attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (rf1clf)\n",
    "\n",
    "plt.barh(range(len(rf1clf.feature_importances_)), rf1clf.feature_importances_)\n",
    "plt.show()\n",
    "\n",
    "print ('Generalization score estimate from training data', clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['maker', 'mileage', 'manufacture_year', 'engine_displacement',\n",
       "       'engine_power', 'body_type', 'color_slug', 'transmission', 'door_count',\n",
       "       'seat_count', 'fuel_type', 'price_eur', 'total_days',\n",
       "       'Maker_alfa-romeo', 'Maker_aston-martin', 'Maker_audi', 'Maker_bentley',\n",
       "       'Maker_bmw', 'Maker_chevrolet', 'Maker_chrysler', 'Maker_citroen',\n",
       "       'Maker_dacia', 'Maker_dodge', 'Maker_fiat', 'Maker_ford', 'Maker_honda',\n",
       "       'Maker_hummer', 'Maker_hyundai', 'Maker_infinity', 'Maker_isuzu',\n",
       "       'Maker_jaguar', 'Maker_jeep', 'Maker_kia', 'Maker_lamborghini',\n",
       "       'Maker_lancia', 'Maker_land-rover', 'Maker_lexus', 'Maker_lotus',\n",
       "       'Maker_maserati', 'Maker_mazda', 'Maker_mercedes-benz', 'Maker_mini',\n",
       "       'Maker_mitsubishi', 'Maker_nissan', 'Maker_opel', 'Maker_peugeot',\n",
       "       'Maker_porsche', 'Maker_renault', 'Maker_rolls-royce', 'Maker_rover',\n",
       "       'Maker_seat', 'Maker_skoda', 'Maker_smart', 'Maker_subaru',\n",
       "       'Maker_suzuki', 'Maker_tesla', 'Maker_toyota', 'Maker_volkswagen',\n",
       "       'Maker_volvo', 'Fuel_cng', 'Fuel_diesel', 'Fuel_electric',\n",
       "       'Fuel_gasoline', 'Fuel_lpg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 82, placement implies 64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-230-e4bf44e737aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    248\u001b[0m                                        raise_cast_failure=True)\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[0;32m   4115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4116\u001b[0m             block = make_block(block, placement=slice(0, len(axis)), ndim=1,\n\u001b[1;32m-> 4117\u001b[1;33m                                fastpath=True)\n\u001b[0m\u001b[0;32m   4118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4119\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[0;32m   2717\u001b[0m                      placement=placement, dtype=dtype)\n\u001b[0;32m   2718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2719\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2721\u001b[0m \u001b[1;31m# TODO: flexible with index=None and/or items=None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim, fastpath)\u001b[0m\n\u001b[0;32m    113\u001b[0m             raise ValueError('Wrong number of items passed %d, placement '\n\u001b[0;32m    114\u001b[0m                              'implies %d' % (len(self.values),\n\u001b[1;32m--> 115\u001b[1;33m                                              len(self.mgr_locs)))\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 82, placement implies 64"
     ]
    }
   ],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lrclf.coef_[0],index=df.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work - Variable reduction using Variance Threshold.\n",
    "\n",
    "For our exceptional work we will use variance threshold to reduce the number of variables and run it through a model on each dataset.  We will slowly increase the varaiance threshold dropping more and more varaible until we hit 50% variance.  We will use random forest model on both datasets since they have a strong result for both datasets.  The random froest will use 30% in 5% increments estimators and no max depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfnew = df\n",
    "dfnew2 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dfnew.values\n",
    "X2 = dfnew2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Random Forest 0% Variance Average accuracy =  82.8490119371 +- 4.62605735496\n",
      "shape is  (81500, 58)\n",
      "Task 2 Random Forest 0% Variance Average accuracy =  28.7100155625 +- 0.521253677912\n",
      "shape is  (81500, 55)\n",
      "Task 1 Random Forest  5.0 % Variance Average accuracy =  81.6038945223 +- 11.0840761902\n",
      "shape is  (81500, 12)\n",
      "Task 2 Random Forest  5.0 % Variance Average accuracy =  24.178860366 +- 5.16665967406\n",
      "shape is  (81500, 12)\n",
      "Task 1 Random Forest  10.0 % Variance Average accuracy =  80.4065151529 +- 14.8321106585\n",
      "shape is  (81500, 11)\n",
      "Task 2 Random Forest  10.0 % Variance Average accuracy =  23.1494089821 +- 5.41061846587\n",
      "shape is  (81500, 11)\n",
      "Task 1 Random Forest  15.0 % Variance Average accuracy =  81.2456497958 +- 11.7997681819\n",
      "shape is  (81500, 10)\n",
      "Task 2 Random Forest  15.0 % Variance Average accuracy =  22.5261227169 +- 5.46675751258\n",
      "shape is  (81500, 10)\n",
      "Task 1 Random Forest  20.0 % Variance Average accuracy =  81.7351702201 +- 10.258092614\n",
      "shape is  (81500, 9)\n",
      "Task 2 Random Forest  20.0 % Variance Average accuracy =  22.7960344324 +- 5.42115117859\n",
      "shape is  (81500, 8)\n",
      "Task 1 Random Forest  25.0 % Variance Average accuracy =  78.6777029063 +- 14.7527440961\n",
      "shape is  (81500, 7)\n",
      "Task 2 Random Forest  25.0 % Variance Average accuracy =  22.7960344324 +- 5.42115117859\n",
      "shape is  (81500, 8)\n",
      "Task 1 Random Forest  30.0 % Variance Average accuracy =  78.6777029063 +- 14.7527440961\n",
      "shape is  (81500, 7)\n",
      "Task 2 Random Forest  30.0 % Variance Average accuracy =  22.7960344324 +- 5.42115117859\n",
      "shape is  (81500, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#*************************************Task 2 Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=50,max_depth=5, random_state=1)\n",
    "acc = cross_val_score(clf,X,y=y,cv=cv)\n",
    "print (\"Task 1 Random Forest 0% Variance Average accuracy = \", acc.mean()*100, \"+-\", acc.std()*100)\n",
    "print('shape is ',dfnew.shape)\n",
    "#************************************Task 2 Random forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "acc = cross_val_score(clf,X2,y=y2,cv=cv)\n",
    "print (\"Task 2 Random Forest 0% Variance Average accuracy = \", acc.mean()*100, \"+-\", acc.std()*10)\n",
    "print('shape is ',dfnew2.shape)\n",
    "for i in range(1,7):\n",
    "    #*************************************Variance threshold \n",
    "    VT = (i*5)/100\n",
    "    percent = VT *100\n",
    "    selector = VarianceThreshold(VT)\n",
    "    selector.fit_transform(dfnew)\n",
    "    idx = selector.get_support()\n",
    "    dfnew = dfnew.loc[:,idx]\n",
    "    X = dfnew.values\n",
    "    #************************************************      \n",
    "    selector = VarianceThreshold(VT)\n",
    "    selector.fit_transform(dfnew2)\n",
    "    idx = selector.get_support()\n",
    "    dfnew2 = dfnew2.loc[:,idx]\n",
    "    X2 = dfnew2.values\n",
    "    #*************************************Task 2 Random Forest\n",
    "    clf = RandomForestClassifier(n_estimators=50,max_depth=5, random_state=1)\n",
    "    acc = cross_val_score(clf,X,y=y,cv=cv)\n",
    "    print (\"Task 1 Random Forest \",percent,\"% Variance Average accuracy = \", acc.mean()*100, \"+-\", acc.std()*100)\n",
    "    print('shape is ',dfnew.shape)\n",
    "    #************************************Task 2 Random forest\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "    acc = cross_val_score(clf,X2,y=y2,cv=cv)\n",
    "    print (\"Task 2 Random Forest \",percent,\"% Variance Average accuracy = \", acc.mean()*100, \"+-\", acc.std()*100)\n",
    "    print('shape is ',dfnew2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above.  When we increase the limit of the threshold of the variance to the columns to drop we loss some of our accuracy.  Our first jump dropped the most features from the datset but we only lost 1% of hte accuracy in the fist task and 4% in the second task.\n",
    "\n",
    "For task 1(mileage) we had a jump in accuracy when we dropped the variance to 20% which had only 10 features in that model. That would be the threshold I would pick for running the predictive model as it would still have a strong accuracy, 1% off from the full feature set, and would be less likely for overfitting.\n",
    "\n",
    "For task 2 (body type) there is a steady decline in accuracy until you  hit 20% variance threshold.  At this point it seems that the data has hit a stable core of features.  I  would pick to run the full model for this predictive model.  Even though it is only a drop of 4% in accuracy for the first step, when the original model has only 28% every percentage point is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
